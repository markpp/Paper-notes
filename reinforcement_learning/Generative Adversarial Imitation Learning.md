# Generative Adversarial Imitation Learning
[arXiv](https://arxiv.org/abs/1606.03476)

### Implementation
[original)](https://github.com/openai/imitation)


### What

- Imitation approaches usually involve a two-stage pipeline: first learning a reward function, then running RL on that reward. Such a pipeline can be slow, and because itâ€™s indirect, it is hard to guarantee that the resulting policy works well. This work shows how one can directly extract policies from data via a connection to GANs. As a result, this approach can be used to learn policies from expert demonstrations (without rewards) on hard OpenAI Gym environments

### How
-

### Experiments
- 
